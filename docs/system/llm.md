# LLM Profile & GPU Notes

## Current Defaults (from code / roadmap)
- **LLM model:** `llama3.1:8b-instruct-q8_0` (set via `LLM_MODEL`)  
- **Embeddings:** `bge-m3` (set via `EMBED_MODEL`)  
- **Ollama host:** `OLLAMA_HOST` (default `http://127.0.0.1:11434`) 

These defaults are fast, low‑VRAM, and sufficient for Tier‑1 features (commands, short replies, startup line). For richer Tier‑2 behavior (nuanced phrasing, steadier longer replies), consider a slightly less‑quantized or slightly larger model if VRAM allows.

## Recommended Model Profiles (pick one)
- **Baseline (fast, light):** `llama3.1:8b-instruct-q4_K_M`  
  Good for uptime, low VRAM, quick answers.
- **Quality bump at 8B:** `llama3.1:8b-instruct-q8_0` (or FP16 if you have headroom)  
  Fewer “compressed” answers, better instruction‑following.
- **Alt small model:** `qwen2.5:7b-instruct` (quantized)  
  Strong instruction‑following; similar latency/VRAM envelope.
- **Bigger GPU path:** `llama3.1:13b-instruct` (light quant or FP16)  
  Better coherence and recall for memory/evolution features.

## Embedding Model Suggestions
- Keep `EMBED_MODEL` configurable.  
- Alternatives that generally improve retrieval quality: `bge-m3`, `gte-small`.

## Env Examples
```env
# Fast & light
LLM_MODEL=llama3.1:8b-instruct-q4_K_M
EMBED_MODEL=bge-m3

# Quality bump at 8B
LLM_MODEL=llama3.1:8b-instruct-q8_0
EMBED_MODEL=bge-m3

# Qwen flavor
LLM_MODEL=qwen2.5:7b-instruct
EMBED_MODEL=bge-m3

## Notes
The chat endpoint is used with a short, strict system prompt for the bot persona and a tiny output (1–2 sentences) to keep replies tight.
The startup line is also generated by the LLM at connect time, with optional STARTUP_EMOTES.
We can toggle models via .env without code changes (that’s already true in index.js)

## Hardware Profile
RTX 4080, 16GB VRAM, Driver 577.00, CUDA 12.9